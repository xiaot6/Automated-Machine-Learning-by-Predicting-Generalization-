{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "from yaml import Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to dataset\n",
    "# change to reflect dataset location\n",
    "\n",
    "train_path = 'split_set/train'\n",
    "test_path = 'split_set/test'\n",
    "val_path = 'split_set/val'\n",
    "\n",
    "architecture_vocab = ['Conv2d', 'Linear', 'MaxPool2d', 'BatchNorm2d', 'Dropout2d', 'ReLU', 'SELU', 'LeakyReLU', 'Flatten', 'Tanh', 'BatchNorm1d', 'Dropout', 'Softmax']\n",
    "# unnecessary as all models use Adam\n",
    "optimizer_vocab = ['SGD', 'Adam', 'Adadelta', 'Adagrad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training set\n",
    "\n",
    "train_data = []\n",
    "\n",
    "for filename in Path(train_path).glob('**/meta_data.yml'):\n",
    "    data_item = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        yamldata = yaml.load(f, Loader=Loader)\n",
    "        data_item['mdl_str'] = yamldata['arch_and_hp']\n",
    "        data_item['train_loss'] = yamldata['train_loss']\n",
    "        data_item['train_error'] = yamldata['train_error']\n",
    "        data_item['val_loss'] = yamldata['val_loss']\n",
    "        data_item['val_error'] = yamldata['val_error']\n",
    "        data_item['test_loss'] = yamldata['test_loss']\n",
    "        data_item['test_error'] = yamldata['test_error']\n",
    "        # model architecture as bag of words, and number of occurences\n",
    "        data_item['mdl_vect'] = [data_item['mdl_str'].count(vocab) for vocab in architecture_vocab] + [data_item['train_loss'], data_item['train_error']]\n",
    "    \n",
    "    train_data.append(data_item)\n",
    "    \n",
    "train_data = np.array(train_data)\n",
    "\n",
    "train_X = []\n",
    "train_y = []\n",
    "\n",
    "for data_item in train_data:\n",
    "    train_X.append(data_item['mdl_vect'])\n",
    "    train_y.append(data_item['test_error'])\n",
    "    \n",
    "train_X = np.array(train_X, dtype=float)\n",
    "train_y = np.array(train_y, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set\n",
    "\n",
    "test_data = []\n",
    "\n",
    "for filename in Path(test_path).glob('**/meta_data.yml'):\n",
    "    data_item = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        yamldata = yaml.load(f, Loader=Loader)\n",
    "        data_item['mdl_str'] = yamldata['arch_and_hp']\n",
    "        data_item['train_loss'] = yamldata['train_loss']\n",
    "        data_item['train_error'] = yamldata['train_error']\n",
    "        data_item['val_loss'] = yamldata['val_loss']\n",
    "        data_item['val_error'] = yamldata['val_error']\n",
    "        data_item['test_loss'] = yamldata['test_loss']\n",
    "        data_item['test_error'] = yamldata['test_error']\n",
    "        # model architecture as bag of words\n",
    "        data_item['mdl_vect'] = [data_item['mdl_str'].count(vocab) for vocab in architecture_vocab] + [data_item['train_loss'], data_item['train_error']]\n",
    "    \n",
    "    test_data.append(data_item)\n",
    "    \n",
    "test_data = np.array(test_data)\n",
    "\n",
    "test_X = []\n",
    "test_y = []\n",
    "\n",
    "for data_item in test_data:\n",
    "    test_X.append(data_item['mdl_vect'])\n",
    "    test_y.append(data_item['test_error'])\n",
    "    \n",
    "test_X = np.array(test_X, dtype=float)\n",
    "test_y = np.array(test_y, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load validation set\n",
    "\n",
    "val_data = []\n",
    "\n",
    "for filename in Path(val_path).glob('**/meta_data.yml'):\n",
    "    data_item = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        yamldata = yaml.load(f, Loader=Loader)\n",
    "        data_item['mdl_str'] = yamldata['arch_and_hp']\n",
    "        data_item['train_loss'] = yamldata['train_loss']\n",
    "        data_item['train_error'] = yamldata['train_error']\n",
    "        data_item['val_loss'] = yamldata['val_loss']\n",
    "        data_item['val_error'] = yamldata['val_error']\n",
    "        data_item['test_loss'] = yamldata['test_loss']\n",
    "        data_item['test_error'] = yamldata['test_error']\n",
    "        # model architecture as bag of words\n",
    "        data_item['mdl_vect'] = [data_item['mdl_str'].count(vocab) for vocab in architecture_vocab] + [data_item['train_loss'], data_item['train_error']]\n",
    "    \n",
    "    val_data.append(data_item)\n",
    "    \n",
    "val_data = np.array(val_data)\n",
    "\n",
    "val_X = []\n",
    "val_y = []\n",
    "\n",
    "for data_item in val_data:\n",
    "    val_X.append(data_item['mdl_vect'])\n",
    "    val_y.append(data_item['test_error'])\n",
    "    \n",
    "val_X = np.array(val_X, dtype=float)\n",
    "val_y = np.array(val_y, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(train_X, train_y)\n",
    "ridge = Ridge(alpha=1.0).fit(train_X, train_y)\n",
    "knn = KNeighborsRegressor(n_neighbors=15).fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Squared Errors for:\n",
      "Linear Regression, Ridge Regression, KNN\n",
      "1.0844433859842815 1.084922497459192 1.4140649898952904\n",
      "\n",
      "Mean Squared Errors for:\n",
      "Linear Regression, Ridge Regression, KNN\n",
      "0.004499765087071708 0.0045017531014904235 0.005867489584627761\n",
      "\n",
      "R2 Scores for :\n",
      "Linear Regression, Ridge Regression, KNN\n",
      "0.6195368262569537 0.619368736087677 0.5038932775211846\n"
     ]
    }
   ],
   "source": [
    "test_y_reg = reg.predict(test_X)\n",
    "reg_sse_err = np.sum((test_y_reg - test_y) ** 2)\n",
    "reg_mse_err = reg_sse_err / test_X.shape[0]\n",
    "reg_r2 = r2_score(test_y, test_y_reg)\n",
    "\n",
    "test_y_ridge = ridge.predict(test_X)\n",
    "ridge_sse_err = np.sum((test_y_ridge - test_y) ** 2)\n",
    "ridge_mse_err = ridge_sse_err / test_X.shape[0]\n",
    "ridge_r2 = r2_score(test_y, test_y_ridge)\n",
    "\n",
    "test_y_knn = knn.predict(test_X)\n",
    "knn_sse_err = np.sum((test_y_knn - test_y) ** 2)\n",
    "knn_mse_err = knn_sse_err / test_X.shape[0]\n",
    "knn_r2 = r2_score(test_y, test_y_knn)\n",
    "\n",
    "print(\"Sum of Squared Errors for:\")\n",
    "print(\"Linear Regression, Ridge Regression, KNN\")\n",
    "print(reg_sse_err, ridge_sse_err, knn_sse_err)\n",
    "print()\n",
    "print(\"Mean Squared Errors for:\")\n",
    "print(\"Linear Regression, Ridge Regression, KNN\")\n",
    "print(reg_mse_err, ridge_mse_err, knn_mse_err)\n",
    "print()\n",
    "print(\"R2 Scores for :\")\n",
    "print(\"Linear Regression, Ridge Regression, KNN\")\n",
    "print(reg_r2, ridge_r2, knn_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pickled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data.p', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = []\n",
    "train_y = []\n",
    "test_X = []\n",
    "test_y = []\n",
    "val_X = []\n",
    "val_y = []\n",
    "\n",
    "for data_item in data:\n",
    "    x = [data_item['meta_data']['arch_and_hp'].count(vocab) for vocab in architecture_vocab]\n",
    "    x += [data_item['meta_data']['train_loss'], data_item['meta_data']['train_error']]\n",
    "    x += data_item['other_path']['train_accs'][:10]\n",
    "    x += data_item['other_path']['train_errors'][:10]\n",
    "    x += data_item['other_path']['train_losses'][:10]\n",
    "    x += data_item['other_path']['val_accs'][:10]\n",
    "    x += data_item['other_path']['val_errors'][:10]\n",
    "    x += data_item['other_path']['val_losses'][:10]\n",
    "    \n",
    "    # Needs work as length of param stats are different for each model\n",
    "#     try:\n",
    "#         x += data_item['param_stats']['init_params_mu']\n",
    "#         x += data_item['param_stats']['final_params_mu']\n",
    "#         x += data_item['param_stats']['init_params_std']\n",
    "#         x += data_item['param_stats']['final_params_std']\n",
    "#         x += data_item['param_stats']['init_params_l2']\n",
    "#         x += data_item['param_stats']['final_params_l2']\n",
    "#     except:\n",
    "#         continue\n",
    "    \n",
    "    y = data_item['meta_data']['test_error']\n",
    "    \n",
    "    if 'train' in data_item['filename']:\n",
    "        train_X.append(x)\n",
    "        train_y.append(y)\n",
    "        \n",
    "    if 'test' in data_item['filename']:\n",
    "        test_X.append(x)\n",
    "        test_y.append(y)\n",
    "        \n",
    "    if 'val' in data_item['filename']:\n",
    "        val_X.append(x)\n",
    "        val_y.append(y)\n",
    "\n",
    "train_X = np.array(train_X, dtype=float)\n",
    "train_y = np.array(train_y, dtype=float)\n",
    "        \n",
    "test_X = np.array(test_X, dtype=float)\n",
    "test_y = np.array(test_y, dtype=float)\n",
    "\n",
    "val_X = np.array(val_X, dtype=float)\n",
    "val_y = np.array(val_y, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(train_X, train_y)\n",
    "ridge = Ridge(alpha=1.0).fit(train_X, train_y)\n",
    "knn = KNeighborsRegressor(n_neighbors=12).fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Squared Errors for:\n",
      "Linear Regression, Ridge Regression, KNN\n",
      "0.45348196007311337 0.5018549249432513 0.9924573495652939\n",
      "\n",
      "Mean Squared Errors for:\n",
      "Linear Regression, Ridge Regression, KNN\n",
      "0.0018816678841208025 0.0020823855806773916 0.004118080288652672\n",
      "\n",
      "R2 Scores for :\n",
      "Linear Regression, Ridge Regression, KNN\n",
      "0.8409016201357192 0.8239305804523988 0.6518089575718093\n"
     ]
    }
   ],
   "source": [
    "test_y_reg = reg.predict(test_X)\n",
    "reg_sse_err = np.sum((test_y_reg - test_y) ** 2)\n",
    "reg_mse_err = reg_sse_err / test_X.shape[0]\n",
    "reg_r2 = r2_score(test_y, test_y_reg)\n",
    "\n",
    "test_y_ridge = ridge.predict(test_X)\n",
    "ridge_sse_err = np.sum((test_y_ridge - test_y) ** 2)\n",
    "ridge_mse_err = ridge_sse_err / test_X.shape[0]\n",
    "ridge_r2 = r2_score(test_y, test_y_ridge)\n",
    "\n",
    "test_y_knn = knn.predict(test_X)\n",
    "knn_sse_err = np.sum((test_y_knn - test_y) ** 2)\n",
    "knn_mse_err = knn_sse_err / test_X.shape[0]\n",
    "knn_r2 = r2_score(test_y, test_y_knn)\n",
    "\n",
    "print(\"Sum of Squared Errors for:\")\n",
    "print(\"Linear Regression, Ridge Regression, KNN\")\n",
    "print(reg_sse_err, ridge_sse_err, knn_sse_err)\n",
    "print()\n",
    "print(\"Mean Squared Errors for:\")\n",
    "print(\"Linear Regression, Ridge Regression, KNN\")\n",
    "print(reg_mse_err, ridge_mse_err, knn_mse_err)\n",
    "print()\n",
    "print(\"R2 Scores for :\")\n",
    "print(\"Linear Regression, Ridge Regression, KNN\")\n",
    "print(reg_r2, ridge_r2, knn_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
